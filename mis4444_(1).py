# -*- coding: utf-8 -*-
"""MIS4444 (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17hSKraiF_mqX9juYSwSvUxnBV4kq4kBw
"""

import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.metrics import mean_absolute_percentage_error
import matplotlib.pyplot as plt
from sklearn.model_selection import GridSearchCV

import pandas as pd
data = pd.read_csv('/content/social.csv')
data.head()

# Display the first few rows of the dataset to understand its structure
data.head()

# Display the first few rows of the dataset to understand its structure
data.head()

data.shape

# Check Null Values
data.isnull().values.any()

print("There is {} missing values in data frame".format(data.isnull().sum().sum()))

print("There is {} duplicated values in data frame".format(data.duplicated().sum()))



# Remove duplicate rows
data.drop_duplicates(inplace=True)

# Reset index after dropping duplicates
data.reset_index(drop=True, inplace=True)

print(f"Number of rows after removing duplicates: {len(data)}")

print("There is {} duplicated values in data frame".format(data.duplicated().sum()))

len(data.columns)

data.describe()

import matplotlib.pyplot as plt
import seaborn as sns

# Identify numerical columns for plotting
numerical_columns = data.select_dtypes(include=np.number).columns

# Create a grid for the plots based on the number of numerical columns
# Assuming a maximum of 4 numerical columns for a 2x2 grid, adjust as needed
num_plots = len(numerical_columns)
if num_plots > 0:
    # Calculate rows and columns for the subplot grid dynamically
    n_cols = min(4, num_plots) # Max 4 columns per row
    n_rows = (num_plots + n_cols - 1) // n_cols # Ceiling division

    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 4 * n_rows))

    # Flatten the axes array to easily iterate over it, even for a single plot
    if n_rows == 1 and n_cols == 1:
        axes = np.array([axes]) # Make it iterable if only one subplot
    axes = axes.flatten()

    # Loop through the numerical columns and plot the histograms
    for i, column in enumerate(numerical_columns):
        if i < len(axes): # Ensure we don't go out of bounds if num_plots < len(axes)
            axes[i].hist(data[column], bins=30, color='skyblue', edgecolor='black')
            axes[i].set_title(f'Distribution of {column}')
            axes[i].set_xlabel(column)
            axes[i].set_ylabel('Frequency')

    # Hide any unused subplots
    for j in range(i + 1, len(axes)):
        fig.delaxes(axes[j])

    # Adjust layout to prevent overlap
    plt.tight_layout()
    plt.show()
else:
    print("No numerical columns found to plot.")

# Check distribution of the target variable
plt.figure(figsize=(20, 12))
sns.histplot(data['Daily_Usage_Time (minutes)'], kde=True, color='blue', bins=50)
plt.title('Distribution of SatisfactionRating')
plt.xlabel('SatisfactionRating')
plt.ylabel('Frequency')
plt.show()

correlation_matrix = data.corr(numeric_only=True)

correlation_matrix

# Check correlations between numeric variables
correlation_matrix = data.corr(numeric_only=True)

# Visualize the correlation matrix using heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix')
plt.show()

# Check for missing values
missing_values = data.isnull().sum()

missing_values

# Step 3: Prepare the features (X) and target (y)
X = data.drop(columns=['Daily_Usage_Time (minutes)','User_ID'])
y = data['Daily_Usage_Time (minutes)']

# Separate numerical and categorical features
numerical_features = X.select_dtypes(include=np.number).columns
categorical_features = X.select_dtypes(include='object').columns

# Create preprocessing pipelines for numerical and categorical features
numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')), # Handle potential missing numerical values
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')), # Handle potential missing categorical values
    ('onehot', OneHotEncoder(handle_unknown='ignore')) # Convert categorical to numerical
])

# Combine preprocessing steps using ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# Apply the preprocessing to X
X_scaled = preprocessor.fit_transform(X)

# Step 5: Split the data into training and testing sets (80-20 split)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Step 6: Apply Random Forest Regressor for prediction
model = RandomForestRegressor(random_state=42)
model.fit(X_train, y_train)

# Step 7: Make predictions and evaluate the model
y_pred = model.predict(X_test)

# Evaluate performance
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
mape = mean_absolute_percentage_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

mae, mse, rmse, mape, r2

svr_model = SVR()
svr_model.fit(X_train, y_train)

# Use the trained SVR model to make predictions on the scaled testing data
y_pred_svr = svr_model.predict(X_test)

# Calculate evaluation metrics for the SVR model
mae_svr = mean_absolute_error(y_test, y_pred_svr)
mse_svr = mean_squared_error(y_test, y_pred_svr)
rmse_svr = np.sqrt(mse_svr)
mape_svr = mean_absolute_percentage_error(y_test, y_pred_svr)
r2_svr = r2_score(y_test, y_pred_svr)

# Print the metrics (optional, but good for verification)
print(f"SVR MAE: {mae_svr}")
print(f"SVR MSE: {mse_svr}")
print(f"SVR RMSE: {rmse_svr}")
print(f"SVR MAPE: {mape_svr}")
print(f"SVR R2: {r2_svr}")

# Define the parameter grid for GridSearchCV for the SVR model
param_grid_svr = {
    'C': [0.1, 1, 10],
    'epsilon': [0.01, 0.1, 0.2],
    'kernel': ['rbf', 'linear']
}

# Instantiate an SVR model with default parameters
svr = SVR()

# Instantiate GridSearchCV with the SVR model, the parameter grid, and 5-fold cross-validation
grid_search_svr = GridSearchCV(svr, param_grid_svr, cv=5)

# Fit the GridSearchCV object to the training data
grid_search_svr.fit(X_train, y_train)

# Access the best SVR model from GridSearchCV
best_svr_model = grid_search_svr.best_estimator_

# Use the best SVR model to make predictions on the scaled test set
y_pred_best_svr = best_svr_model.predict(X_test)

# Calculate evaluation metrics for the best SVR model
mae_best_svr = mean_absolute_error(y_test, y_pred_best_svr)
mse_best_svr = mean_squared_error(y_test, y_pred_best_svr)
rmse_best_svr = np.sqrt(mse_best_svr)
mape_best_svr = mean_absolute_percentage_error(y_test, y_pred_best_svr)
r2_best_svr = r2_score(y_test, y_pred_best_svr)

# Print the metrics
print(f"Best SVR MAE: {mae_best_svr}")
print(f"Best SVR MSE: {mse_best_svr}")
print(f"Best SVR RMSE: {rmse_best_svr}")
print(f"Best SVR MAPE: {mape_best_svr}")
print(f"Best SVR R2: {r2_best_svr}")

from sklearn.neighbors import KNeighborsRegressor

knn_model = KNeighborsRegressor(n_neighbors=5)

knn_model.fit(X_train, y_train)

y_pred_knn = knn_model.predict(X_test)

mae_knn = mean_absolute_error(y_test, y_pred_knn)
mse_knn = mean_squared_error(y_test, y_pred_knn)
rmse_knn = np.sqrt(mse_knn)
mape_knn = mean_absolute_percentage_error(y_test, y_pred_knn)
r2_knn = r2_score(y_test, y_pred_knn)

print(f"KNN MAE: {mae_knn}")
print(f"KNN MSE: {mse_knn}")
print(f"KNN RMSE: {rmse_knn}")
print(f"KNN MAPE: {mape_knn}")
print(f"KNN R2: {r2_knn}")

# Define the parameter grid for GridSearchCV
param_grid = {
    'n_neighbors': [3, 5, 7, 9, 11]
}

# Instantiate a KNeighborsRegressor with default parameters
knn = KNeighborsRegressor()

# Instantiate GridSearchCV with the KNN model, the parameter grid, and 5-fold cross-validation
grid_search = GridSearchCV(knn, param_grid, cv=5)

# Fit the GridSearchCV object to the training data
grid_search.fit(X_train, y_train)

# Extract the best n_neighbors value
best_n_neighbors = grid_search.best_params_['n_neighbors']

# Extract the best KNN model
best_knn_model = grid_search.best_estimator_

# Use the best model to make predictions on the test set
y_pred_best_knn = best_knn_model.predict(X_test)

# Calculate evaluation metrics for the best KNN model
mae_best_knn = mean_absolute_error(y_test, y_pred_best_knn)
mse_best_knn = mean_squared_error(y_test, y_pred_best_knn)
rmse_best_knn = np.sqrt(mse_best_knn)
mape_best_knn = mean_absolute_percentage_error(y_test, y_pred_best_knn)
r2_best_knn = r2_score(y_test, y_pred_best_knn)

# Print the metrics
print(f"Best KNN MAE: {mae_best_knn}")
print(f"Best KNN MSE: {mse_best_knn}")
print(f"Best KNN RMSE: {rmse_best_knn}")
print(f"Best KNN MAPE: {mape_best_knn}")
print(f"Best KNN R2: {r2_best_knn}")

xgb_model = XGBRegressor(random_state=42)

xgb_model.fit(X_train, y_train)

y_pred_xgb = xgb_model.predict(X_test)

mae_xgb = mean_absolute_error(y_test, y_pred_xgb)
mse_xgb = mean_squared_error(y_test, y_pred_xgb)
rmse_xgb = np.sqrt(mse_xgb)
mape_xgb = mean_absolute_percentage_error(y_test, y_pred_xgb)
r2_xgb = r2_score(y_test, y_pred_xgb)

print(f"XGBoost MAE: {mae_xgb}")
print(f"XGBoost MSE: {mse_xgb}")
print(f"XGBoost RMSE: {rmse_xgb}")
print(f"XGBoost MAPE: {mape_xgb}")
print(f"XGBoost R2: {r2_xgb}")

# Define the parameter grid for GridSearchCV for XGBoost
param_grid_xgb = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}

# Instantiate an XGBRegressor
xgb = XGBRegressor(random_state=42)

# Instantiate GridSearchCV with the XGBoost model, the parameter grid, and 5-fold cross-validation
grid_search_xgb = GridSearchCV(xgb, param_grid_xgb, cv=5)

# Fit the GridSearchCV object to the training data
grid_search_xgb.fit(X_train, y_train)

# Access the best hyperparameters found by GridSearchCV
best_params_xgb = grid_search_xgb.best_params_

# Access the best estimator (the trained XGBoost model)
best_xgb_model = grid_search_xgb.best_estimator_

# Print the best hyperparameters
print("Best XGBoost Hyperparameters:", best_params_xgb)

# Use the best XGBoost model to make predictions on the test set
y_pred_best_xgb = best_xgb_model.predict(X_test)

# Calculate evaluation metrics for the best XGBoost model
mae_best_xgb = mean_absolute_error(y_test, y_pred_best_xgb)
mse_best_xgb = mean_squared_error(y_test, y_pred_best_xgb)
rmse_best_xgb = np.sqrt(mse_best_xgb)
mape_best_xgb = mean_absolute_percentage_error(y_test, y_pred_best_xgb)
r2_best_xgb = r2_score(y_test, y_pred_best_xgb)

# Print the metrics
print(f"Best XGBoost MAE: {mae_best_xgb}")
print(f"Best XGBoost MSE: {mse_best_xgb}")
print(f"Best XGBoost RMSE: {rmse_best_xgb}")
print(f"Best XGBoost MAPE: {mape_best_xgb}")
print(f"Best XGBoost R2: {r2_best_xgb}")

# Get feature importances from the best XGBoost model
feature_importances = best_xgb_model.feature_importances_

# Get the names of the features after preprocessing
# This correctly accounts for one-hot encoded categorical features
processed_feature_names = preprocessor.get_feature_names_out()

# Create a DataFrame to store feature names and their importances
feature_importance_df = pd.DataFrame({'feature': processed_feature_names, 'importance': feature_importances})

# Sort the features by importance in descending order
feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)

# Display the feature importances
print("Feature Importances (XGBoost):")
display(feature_importance_df)

plt.figure(figsize=(12, 8))
sns.barplot(x='importance', y='feature', data=feature_importance_df, color='khaki', edgecolor='black')
plt.title('Feature Importance from XGBoost Model')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.show()

from sklearn.inspection import permutation_importance

# Calculate permutation importance for the best SVR model
# Convert X_test to a dense array as permutation_importance expects array-like input
perm_importance_svr = permutation_importance(best_svr_model, X_test.toarray(), y_test, random_state=42)

# Get the feature importances and sort them
sorted_idx = perm_importance_svr.importances_mean.argsort()

# Get the names of the features after preprocessing
processed_feature_names = preprocessor.get_feature_names_out()

feature_importance_svr_df = pd.DataFrame({
    'feature': processed_feature_names[sorted_idx],
    'importance': perm_importance_svr.importances_mean[sorted_idx]
})

from sklearn.inspection import permutation_importance

# Calculate permutation importance for the best SVR model
# Convert X_test to a dense array as permutation_importance expects array-like input
perm_importance_svr = permutation_importance(best_svr_model, X_test.toarray(), y_test, random_state=42);

# Get the feature importances and sort them
sorted_idx = perm_importance_svr.importances_mean.argsort()

# Get the names of the features after preprocessing
processed_feature_names = preprocessor.get_feature_names_out()

feature_importance_svr_df = pd.DataFrame({
    'feature': processed_feature_names[sorted_idx],
    'importance': perm_importance_svr.importances_mean[sorted_idx]
})

plt.figure(figsize=(10, 10))
sns.barplot(x='importance', y='feature', data=feature_importance_svr_df, color='lightblue', edgecolor='black')
plt.title('Permutation Importance from Optimized SVR Model')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.show()

#Pre-Step: Prepare Data for Splitting


from sklearn.preprocessing import LabelEncoder

# 1. Define Features (X) and Target (y)
# Dropping User_ID as it's an identifier, not a predictor
X = data.drop(columns=['User_ID', 'Dominant_Emotion'])
y = data['Dominant_Emotion']

# 2. Encode Categorical Data
# Convert 'Age' to numeric if it was read as object/category
X['Age'] = pd.to_numeric(X['Age'], errors='coerce')

# One-Hot Encoding for categorical features (Gender, Platform)
X = pd.get_dummies(X, columns=['Gender', 'Platform'], drop_first=True)

# Label Encoding for the Target Variable
le = LabelEncoder()
y = le.fit_transform(y)

print("Features shape:", X.shape)
print("Target shape:", y.shape)

#Step 7: Split the Data


from sklearn.model_selection import train_test_split

# Split data: 80% Training, 20% Testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"Training set size: {X_train.shape}")
print(f"Testing set size: {X_test.shape}")

#Step 8: Scaling (Normalization / Standardization)

from sklearn.preprocessing import StandardScaler

# Initialize Scaler
scaler = StandardScaler()

# Fit on training set only, then transform both
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Convert back to DataFrame for easier handling (optional but helpful)
X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns)
X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns)

#Step 9: Feature Selection

from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.impute import SimpleImputer

# Handle potential NaNs in the scaled data before feature selection
imputer = SimpleImputer(strategy='mean')
X_train_scaled_imputed = imputer.fit_transform(X_train_scaled)
X_test_scaled_imputed = imputer.transform(X_test_scaled)

# Select top 10 features (or fewer depending on your data columns)
# Using ANOVA F-value since inputs are numerical and target is categorical
selector = SelectKBest(score_func=f_classif, k='all') # Set k=5 or 'all'
X_train_selected = selector.fit_transform(X_train_scaled_imputed, y_train)
X_test_selected = selector.transform(X_test_scaled_imputed)

# Get selected feature names
selected_indices = selector.get_support(indices=True)
selected_features = X.columns[selected_indices]
print("Selected Features:", selected_features.tolist())

#Step 10, 11, 12: Model Build, Train, and Test

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Dictionary to store models
models = {
    "Logistic Regression": LogisticRegression(),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "Support Vector Machine": SVC(probability=True, random_state=42)
}

# Dictionary to store predictions
predictions = {}

for name, model in models.items():
    # Step 10 & 11: Build and Train
    model.fit(X_train_selected, y_train)

    # Step 12: Model Test
    y_pred = model.predict(X_test_selected)
    predictions[name] = y_pred
    print(f"{name} trained and tested.")

#Step 13: Model Evaluate

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import seaborn as sns

results = {}

for name, y_pred in predictions.items():
    acc = accuracy_score(y_test, y_pred)
    results[name] = acc

    print(f"--- {name} ---")
    print(f"Accuracy: {acc:.4f}")
    print(classification_report(y_test, y_pred, target_names=le.classes_, labels=le.transform(le.classes_)))

    # Optional: Plot Confusion Matrix
    # cm = confusion_matrix(y_test, y_pred)
    # sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    # plt.title(f'Confusion Matrix: {name}')
    # plt.show()
    print("\n")

#Step 14: Compare and Choose the Best Model

# 1. Compare models
best_model_name = max(results, key=results.get)
best_model = models[best_model_name]
print(f"ðŸ† Best Model: {best_model_name} with Accuracy: {results[best_model_name]:.4f}")

# 2. Feature Importance (if applicable, e.g., Random Forest)
if best_model_name == "Random Forest":
    importances = best_model.feature_importances_
    feature_importance_df = pd.DataFrame({
        'Feature': selected_features,
        'Importance': importances
    }).sort_values(by='Importance', ascending=False)

    print("\nFeature Importance:")
    print(feature_importance_df)

    # Plot Feature Importance
    plt.figure(figsize=(10, 6))
    sns.barplot(x='Importance', y='Feature', data=feature_importance_df)
    plt.title('Feature Importance for Best Model')
    plt.show()
else:
    print(f"Feature importance plot not available for {best_model_name} (only available for tree-based models).")

#Step 15: Save the Best Model

import joblib

# Save the model
filename = 'best_model.pkl'
joblib.dump(best_model, filename)

# Also save the scaler and label encoder for preprocessing new data later
joblib.dump(scaler, 'scaler.pkl')
joblib.dump(le, 'label_encoder.pkl')

print(f"Model saved as {filename}")